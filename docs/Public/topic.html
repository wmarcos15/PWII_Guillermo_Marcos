<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Guillermo Marcos Lara</title>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link href="../css/style.css" rel="stylesheet">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	</head>
	<body>

		<!-- Navegation bar -->
		<header class="navbar">
			<div class="home">
				<a href="../index.html" class="fa fa-home"></a>
			</div>
			<div class="pages">
				<a href="./about.html">About me</a>
				<a href="./topic.html">Topic</a>
				<a href="./net.html">Net</a>
				<a href="./degree.html">Degree</a>
				<a href="./contact.html">Contact me!</a>
			</div>
		</header>

		<div class="topic">
			<div class="topic-title">
				<h1>Large Language <br>Models</h1>
				<h3>Comprehending their importance and functionality</h3>
			</div>
		</div>

		<div class="transformers">
			<div class="transformers-text">
				<h1>Transformers</h1>
				<p>Transformers are neural networks that operate based on selecting data and examining its relationships with other data, words, or sentences, so they can improve the understanding and production of text in  LLM. Transformers uses many operations such as word embeddings and attention levels. It all started with the publication of an article edited by Vaswani titled "Attention is all you need". A new structure of LLM and transformers is established to explain how  attention works. We used RNN and CNN self-attention mechanisms, which would otherwise reduce training time. The use of self-attention mechanisms has resulted in several advantages. Some important features of transformers are that they can manipulate remote information and do not use iterative connections like RNNs. That is, it is efficient when working with large amounts of data. To obtain the result, the transformer assigns the input vector to an output vector of similar length. Within Transformers, there are several blocks that contain networks with several technologies, and the self-attention layer is one of the most important. This is because you can leverage information from large amounts of data without using regular connections.</p>
				<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Click here to read more about transformers</a>
			</div>
			<img src="../Images/transformer.png" alt="">
		</div>

	</body>
</html>
